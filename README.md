# QOR_Project
## Project Overview
This project parses log files, stores parsed data in a MySQL database, exposes this data through a Flask-based API, and automates the testing and deployment process with Docker and CI/CD. 

The project consists of several parts:
- Parsing Script: Reads and processes log files to generate a fermi.txt file.
- Recording Script: Stores parsed data from fermi.txt into a MySQL database.
- API Script: Exposes data via a Flask API.
- Dockerized Application: Containerizes the entire application with Docker.
- API Testing: Automates API testing and email reports using Postman and Python.
- CI/CD Pipeline: Automates deployment with GitHub Actions, including Slack notifications.

Technologies Used
- Python: For scripting (parsing, recording, API, sending email reports).
- Flask: For building the API.
- MySQL: Database for storing parsed data.
- Docker: Containerization for both the Flask app and MySQL.
- Postman: For automated API testing.
- Newman: For running Postman tests in the CI/CD pipeline.
- GitHub Actions: For CI/CD automation.
- msmtp: For sending email notifications.

## Log File Parsing

### Parsing Script: `parsing.py`

The `parsing.py` script is designed to parse log files generated by the system, extract relevant statistics, and categorize them into different sections. It then saves these statistics to a `fermi.txt` file located in the `qor` folder for each job ID.

### Features:
- Parses the log file to categorize various statistics into the following groups:
  - **Main Stats**
  - **Runtime Analysis Stats**
  - **Geometric Analysis Stats**
  - **Statistical Analysis (multiple subcategories)**
- Saves the parsed data into a `fermi.txt` file in the respective `qor` folder of the job.
- Supports copying the `qor` folder to a predefined rsync directory.

### Usage:

To use the script, you need to run it from the command line with specific arguments. Here's the general syntax for using the script:

```bash
python parsing.py -l <location> -id <job_id> -p [-r]
```
## Database Recording

### Recording Script: `recording.py`

The `recording.py` script is responsible for processing job data and recording it into a MySQL database. It reads the `fermi.txt` files generated by the log parsing script and inserts the corresponding statistics into various database tables. The script handles job metadata, runtime statistics, geometric analysis, and statistical analysis results, organizing them into appropriate database structures.

### Features:
- Reads `fermi.txt` files from job folders.
- Extracts relevant statistics from each file.
- Inserts or updates records in a MySQL database.
- Supports processing of all job folders or a specific job ID.
- Uses SQLAlchemy ORM for database interaction, ensuring ease of use and scalability.
- Configures detailed logging for tracking the process.

### Database Schema:

The script inserts data into multiple related tables:
- **MetaData**: Contains job metadata such as Fermi ID, Fermi Name, and Revision Commit.
- **Main_Stats**: Stores main statistical data for each job.
- **Geometric_Analysis_Stats**: Stores geometric analysis statistics.
- **Runtime_Analysis_Stats**: Stores runtime-related statistics.
- **Statistical_Analysis**: Several sub-tables store statistical analysis data for different parameters.

Each table is linked to the `MetaData` table through the `UniqueKey`, ensuring data integrity and proper relationship between statistics and their associated job metadata.

### Usage:

To use the script, you need to run it from the command line with specific arguments. Here's the general syntax for using the script:

```bash
python recording.py [-id <job_id>]
```
